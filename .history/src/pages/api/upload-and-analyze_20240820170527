import { NextApiRequest, NextApiResponse } from 'next';
import formidable, { Files, Fields } from 'formidable';
import { MongoClient } from 'mongodb';
import { createReadStream, unlink } from 'fs';
import { promises as fs } from 'fs';
import pdfParse from 'pdf-parse';
import OpenAI from 'openai';

// Initialize OpenAI API
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!,
});

// MongoDB setup
const MONGODB_URI = process.env.MONGODB_URI!;
const MONGODB_DB_NAME = 'reportData';

let client: MongoClient | null = null;

const connectToDatabase = async () => {
  if (!client) {
    client = new MongoClient(MONGODB_URI, {
      useNewUrlParser: true,
      useUnifiedTopology: true,
    });
    await client.connect();
  }
  return client.db(MONGODB_DB_NAME);
};

// Formidable setup for handling file uploads
interface FormidableResult {
  fields: Fields;
  files: Files;
}

const parseForm = (req: NextApiRequest): Promise<FormidableResult> => {
  const form = new formidable.IncomingForm();
  form.parse(req, (err, fields, files) => {
    if (err) {
      throw err;
    }
  });
  return new Promise((resolve, reject) => {
    form.parse(req, (err, fields, files) => {
      if (err) reject(err);
      resolve({ fields, files });
    });
  });
};

// PDF text extraction
const extractTextFromPDF = async (fileStream: NodeJS.ReadableStream): Promise<string> => {
  const dataBuffer = await new Promise<Buffer>((resolve, reject) => {
    const chunks: Buffer[] = [];
    fileStream.on('data', (chunk) => chunks.push(chunk));
    fileStream.on('end', () => resolve(Buffer.concat(chunks)));
    fileStream.on('error', reject);
  });
  const data = await pdfParse(dataBuffer);
  return data.text;
};

// OpenAI analysis
const fetchOpenaiAnalysis = async (textData: string): Promise<string> => {
  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-3.5-turbo', // or another model if required
      messages: [{
        role: 'user',
        content: `Analyze the following medical report and identify any potential health issues. Specify the problem, the associated body part, and its severity (low, moderate, high):\n\n${textData}`,
      }],
      max_tokens: 1000,
    });

    // Check if choices array is not empty and return the content
    if (completion.choices.length > 0 && completion.choices[0].message) {
      return completion.choices[0].message.content.trim();
    } else {
      return 'No analysis provided.';
    }
  } catch (error) {
    console.error('Error fetching OpenAI analysis:', error);
    return 'Error fetching analysis.';
  }
};

// API route handler
export default async function handler(req: NextApiRequest, res: NextApiResponse) {
  if (req.method !== 'POST') {
    return res.status(405).json({ message: 'Method Not Allowed' });
  }

  try {
    const db = await connectToDatabase();
    const { files } = await parseForm(req);

    if (!files.file || (Array.isArray(files.file) && files.file.length === 0)) {
      return res.status(400).json({ message: 'No file uploaded' });
    }

    const file = Array.isArray(files.file) ? files.file[0] : files.file;
    const filePath = file.filepath;
    const fileStream = createReadStream(filePath);

    const textData = await extractTextFromPDF(fileStream);
    const analysis = await fetchOpenaiAnalysis(textData);

    // Optionally, store file metadata and analysis in MongoDB
    await db.collection('uploads').insertOne({
      filename: file.originalFilename,
      filepath: filePath,
      size: file.size,
      type: file.mimetype,
      analysis,
      createdAt: new Date(),
    });

    await fs.unlink(filePath);  // Clean up uploaded file

    res.status(200).json({ insights: analysis });
  } catch (error) {
    console.error('Server error:', error);
    res.status(500).json({ message: 'Internal server error', error: error.message });
  }
}
